{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Tokenization, Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will focus on tokenization of the input text, then performing the stemming and lemmatization techniques we learnt in this session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the file/s from the data folder and load it into memory using python, pandas or any python fashion you are comfortable with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Corporate-messaging-DFE.csv', sep=',', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3118 entries, 0 to 3117\n",
      "Data columns (total 11 columns):\n",
      "_unit_id               3118 non-null int64\n",
      "_golden                3118 non-null bool\n",
      "_unit_state            3118 non-null object\n",
      "_trusted_judgments     3118 non-null int64\n",
      "_last_judgment_at      2811 non-null object\n",
      "category               3118 non-null object\n",
      "category:confidence    3118 non-null float64\n",
      "category_gold          307 non-null object\n",
      "id                     3118 non-null float64\n",
      "screenname             3118 non-null object\n",
      "text                   3118 non-null object\n",
      "dtypes: bool(1), float64(2), int64(2), object(6)\n",
      "memory usage: 246.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Information', 'Action', 'Dialogue', 'Exclude'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset consists of four caterogies\n",
    "1. Information\n",
    "2. Action\n",
    "3. Dialogue\n",
    "4. Exlude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['_golden'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['finalized', 'golden'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['_unit_state'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step, we need to \n",
    "- tokenize the text into different tokens\n",
    "- Eliminate tokens and candidates we dont care about - like stopwords, punctuations, numbers etc\n",
    "- Create a clean list of tokens for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to remove the http at the end of all the documents\n",
    "def remove_link(sentence):\n",
    "    '''\n",
    "    takes in a sentence\n",
    "    \n",
    "    returns a the sentennce with link at the removed\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    doc1 = [item.lower() for item in sentence.strip().split('http')[:-1]]\n",
    "    doc2 = ''.join(doc1).split()\n",
    "    \n",
    "    return doc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the http from all the sentences \n",
    "df['text1'] = df['text'].map(remove_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_stopwords(word):\n",
    "    return([w for w in word if not w in stopwords_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing punctuation\n",
    "import string\n",
    "punctuation_ = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokernize\n",
    "def cleaner(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in nltk.corpus.stopwords.words('english')]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting the tokens\n",
    "df1['tokens'] = df1['text'].map(cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stemming using NLTK : http://www.nltk.org/howto/stem.html\n",
    "- Lemmatization using NLTK lemmatizer : https://pythonprogramming.net/lemmatizing-nltk-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer_porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drunk'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer_porter.stem('drunking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "def stemming(tokens):\n",
    "    return [stemmer_porter.stem(token) for token in tokens]\n",
    "\n",
    "df1['tokens_stem'] = df1['tokens'].map(stemming)\n",
    "#df1['tokens'].map([stemmer_porter.stem(token) for token in df1['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [barclay, ceo, stress, import, regulatori, cul...\n",
       "1       [barclay, announc, result, right, issu, http, ...\n",
       "2       [barclay, publish, prospectu, å£5.8bn, right, ...\n",
       "3       [barclay, group, financ, director, chri, luca,...\n",
       "4       [barclay, announc, iren, mcdermott, brown, app...\n",
       "5       [barclay, respons, pra, capit, shortfal, exerc...\n",
       "6       [barclay, sponsor, #, zamynforum, bbc, world, ...\n",
       "7       [barclay, today, publish, respons, salz, revie...\n",
       "8       [read, statement, #, barclay, ceo, bonu, award...\n",
       "9       [59, %, worker, either, look, chang, job, appl...\n",
       "10      [longer, one, workforc, ., five, ., want, empl...\n",
       "11      [uk, entrepreneuri, activ, 2013, glanc, -, bar...\n",
       "12      [emma, turner, ,, head, client, philanthropi, ...\n",
       "13      [..., visit, us, @, napfnew, 5-7, march, workf...\n",
       "14      [chill, #, emergingmarket, last, littl, longer...\n",
       "15      [sinc, 2004, 've, invest, å£37.5m, #, spacesfo...\n",
       "16      [barclay, servic, execut, :, help, feel, secur...\n",
       "17      [jaim, arguello, examin, whether, 2014, right,...\n",
       "18      [student, year, award, island, 's, except, you...\n",
       "19      [young, peopl, focu, 4, local, chariti, award,...\n",
       "20      [delight, rank, best, wealth, manag, social, 2...\n",
       "21      [barclay, &, amp, @, bgf_team, #, entrepreneur...\n",
       "22      [proud, sponsor, @, bvca, confer, today, ., lo...\n",
       "23      [ûïemploy, must, address, fundament, landscap...\n",
       "24      [gen, :, 65, %, seek, financi, educ, &, amp, g...\n",
       "25      [visit, us, stand, 310, @, eb__liv, see, new, ...\n",
       "26      [benefit, strategi, fail, younger, worker, ., ...\n",
       "27      [gen, x, place, import, compani, pension, sche...\n",
       "28      [babi, boomer, :, least, demand, gener, ,, 71,...\n",
       "29      [latest, research, find, 90, %, employe, gener...\n",
       "                              ...                        \n",
       "3088    [h4hiniti, develop, strategi, encourag, patien...\n",
       "3089    [intern, osteoporosi, foundat, @, iofbonehealt...\n",
       "3090    [world, wide, web, 25, today, &, amp, ;, go, s...\n",
       "3091    [heart, health, month, ,, million, heart, begi...\n",
       "3092    [week, @, sleepfoundationì¢ââã¢, sleep, awar...\n",
       "3093    [celebr, global, commun, day, 400, #, citivolu...\n",
       "3094    [today, intern, matern, health, day, ., rt, ag...\n",
       "3095    [today, cmo, join, panel, nation, healthcar, i...\n",
       "3096    [today, ,, health, ministri, reward, danon, sp...\n",
       "3097    [turn, wast, gold, !, see, @, sanergi, solv, 1...\n",
       "3098    [want, improv, health, ?, read, import, follow...\n",
       "3099    [watch, celebr, @, chefartsmith, cook, #, heal...\n",
       "3100    [watch, move, stori, @, parinaam, assist, poor...\n",
       "3101    [watch, globe, 2nite, ?, donì¢ââã¢t, miss, @...\n",
       "3102    [honor, select, 2014, manufactur, leadership, ...\n",
       "3103    [moment, away, learn, name, #, ftcitiaward, wi...\n",
       "3104    [proud, recipi, #, citizens2013, award, !, lea...\n",
       "3105    [pledg, acceler, #, salt, reduct, food, brand,...\n",
       "3106    [think, 's, shame, ``, cake, bake, '', n't, ma...\n",
       "3107    ['re, grate, 2x, honor, @, chamberbclc, citize...\n",
       "3108    [weì¢ââã¢r, 1., sign, european, initi, incre...\n",
       "3109    [weì¢ââã¢r, work, hard, promot, healthier, l...\n",
       "3110    [wish, children, ate, healthier, ?, see, tree,...\n",
       "3111    [wish, citi, child-friendli, ?, see, aproch, h...\n",
       "3112    [wish, follow, happi, healthi, new, year, !, w...\n",
       "3113       [wish, happi, healthi, thanksgiv, ., #, bewel]\n",
       "3114    [wouldnì¢ââã¢t, great, oven, recognis, food,...\n",
       "3115    [yesterday, ,, #, healthykid, lit, broadway, #...\n",
       "3116    [yo-jelli, ,, danon, new, brand, south, africa...\n",
       "3117    [z, bhutta, :, problem, food, &, amp, ;, land,...\n",
       "Name: tokens_stem, Length: 3118, dtype: object"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lammitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer_porter = PorterStemmer()\n",
    "tokens_stemporter = [list(map(stemmer_porter.stem, sent)) for sent in tokens_filtered]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "Stemming and Lemmatization using spacy: https://spacy.io/usage/spacy-101#annotations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
